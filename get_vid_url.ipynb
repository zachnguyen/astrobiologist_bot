{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import os, json\n",
    "import requests\n",
    "\n",
    "\n",
    "API_KEY = os.getenv(\"YTB_API_KEY\")\n",
    "\n",
    "# Initialize the YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Define the channel ID or username\n",
    "channel_name = \"nasaastrobiology\"  # You can also use a channel's username\n",
    "video_urls = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_id(channel_name):\n",
    "    \"\"\"\n",
    "    Retrieves the id of a youtube channel from its channel name.\n",
    "\n",
    "    Args:\n",
    "      channel_name: Name of the youtube channel which is not the full name of channel but the name after the '@'\n",
    "                    the channel link.\n",
    "\n",
    "    Returns:\n",
    "      The id of of the given channel.\n",
    "    \"\"\"\n",
    "    url = \"https://www.youtube.com/@\" + channel_name\n",
    "    r = requests.get(url)\n",
    "    # Retrieve the whole page source\n",
    "    text = r.text\n",
    "    # Split the text to get only the section containing the channel id\n",
    "    id = text.split(\"youtube.com/channel/\")[1].split('\">')[0]\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_video_ids(channel_name):\n",
    "    \"\"\"\n",
    "    Fetches the video IDs of the videos in the uploads playlist of a channel.\n",
    "    Args:\n",
    "      channel_name: The name of the channel.\n",
    "    Returns:\n",
    "      A list of {video ID, video url, title}.\n",
    "    \"\"\"\n",
    "    # Make a request to youtube api\n",
    "    base_url = \"https://www.googleapis.com/youtube/v3/channels\"\n",
    "    channel_id = get_channel_id(channel_name)\n",
    "    params = {\"part\": \"contentDetails\", \"id\": channel_id, \"key\": API_KEY}\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response = json.loads(response.content)\n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "    if \"items\" not in response or not response[\"items\"]:\n",
    "        raise Exception(f\"No playlist found for {channel_name}\")\n",
    "\n",
    "    # Retrieve the uploads playlist ID for the given channel\n",
    "    playlist_id = response[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "    # Retrieve all videos from uploads playlist\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        playlist_items_response = (\n",
    "            youtube.playlistItems()\n",
    "            .list(\n",
    "                # part=\"contentDetails\",\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "        videos += playlist_items_response[\"items\"]\n",
    "\n",
    "        next_page_token = playlist_items_response.get(\"nextPageToken\")\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    # Extract video URLs\n",
    "    video_urls = []\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
    "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        video_title = video[\"snippet\"][\"title\"]\n",
    "        video_published_ts = video[\"snippet\"][\"publishedAt\"]\n",
    "        video_urls.append({\"ID\": video_id, \"URL\": video_url, \"Title\": video_title, \"Published At\": video_published_ts})\n",
    "\n",
    "    return video_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = fetch_video_ids(channel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter URLs based on the \"Published At\" year\n",
    "urls = [video['URL'] for video in urls if video['Published At'][:4] in ['2023', '2024']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_with_channel = [url + \"&ab_channel=NASAAstrobiology\" for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasaastrobiology.txt'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the URLs to a txt file\n",
    "file_path = 'nasaastrobiology.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    for url in urls_with_channel:\n",
    "        file.write(url + '\\n')\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
